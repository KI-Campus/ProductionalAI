{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this module we want to dive into process monitoring. While it is possible to use AI to do this, we want to use a more simple but yet effective approach. In order to get some hands on experience, we will use data from the IEEE PHM 2023 Prognostic challenge. The goal is to detect bearing degradation (so this is an anomaly detection problem). We have vibration signals (horizontal and vertical) for all bearings. We have a training set with data from 'normal' bearings, as well as a test set (with 'normal' and degraded bearings). We want to detect the faulty bearings in the test set.\n",
    "The sensor sampling rate is 25.6 kHz. We will use a confidence level of 0.95 for the anomaly detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.signal import hilbert, butter, filtfilt\n",
    "import scipy.stats as st\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "sampling_rate = 25600\n",
    "SENSOR = 'horizontal'   # horizontal / vertical\n",
    "CONFIDENCE_LVL = 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will load the training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and test data\n",
    "with open('train_data_pkl', 'rb') as f:\n",
    "    train_data = pickle.load(f)\n",
    "with open('test_data_pkl', 'rb') as f:\n",
    "    test_data = pickle.load(f)\n",
    "\n",
    "for idx, data in enumerate(train_data):\n",
    "    if SENSOR == 'horizontal':\n",
    "        train_data[idx] = data[:,4]\n",
    "    else:\n",
    "        train_data[idx] = data[:,5]\n",
    "for idx, data in enumerate(test_data):\n",
    "    if SENSOR == 'horizontal':\n",
    "        test_data[idx] = data[:,4]\n",
    "    else:\n",
    "        test_data[idx] = data[:,5]\n",
    "\n",
    "print(f'Size training set: {len(train_data)}')\n",
    "print(f'Size training set: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us have a look at the training data. It should only contain data from good bearings that haven't degraded yet. Let us first look at data from a single bearing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization of single bearing from training dataset\n",
    "plt.plot(train_data[1], c='grey', alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will visualize the whole training dataset (500 bearings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization of training data\n",
    "for data_bad_part in train_data:\n",
    "    plt.plot(data_bad_part, c='grey', alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good! The vibration sensor is not exactly the same for every bearing, but that is completely normal. Now let's look at the test data. Besides data from good bearings, the dataset should also contain data from degraded bearings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization of test data\n",
    "for data_bad_part in test_data:\n",
    "    plt.plot(data_bad_part, c='grey', alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test set is significantly smaller, so we are seeing less extreme peaks. We will try to set up a system that is able to detect degraded bearings automatically. We will create envelopes using the training data. This allows us to detect anomalies (degraded bearings) by comparing test data to the envelopes. First, we will create some methods in order to create an envelope from the training data.\n",
    "We are applying a bandpass filter for preprocessing the time series data. After that, we will use the hilbert transformation for creating an envelope. We are creating as many envelopes as we have bearings in the training dataset. In the last step, we combine all envelopes using the mean for each time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bandpass filtering\n",
    "def bandpass_filter(data, lowcut, highcut, order=4):\n",
    "    b, a = butter(order, [lowcut, highcut], btype='band', fs=sampling_rate)\n",
    "    return filtfilt(b, a, data)\n",
    "\n",
    "def preprocessing(input_data):\n",
    "    input_data_filtered = bandpass_filter(input_data, lowcut=1000, highcut=10000)\n",
    "    return np.absolute(input_data_filtered)\n",
    "\n",
    "# hilbert transformation\n",
    "def create_single_envelope(input_signal):\n",
    "    transformed_signal = hilbert(input_signal)\n",
    "    envelope = np.abs(transformed_signal)\n",
    "    return envelope\n",
    "\n",
    "# train envelope from multiple curves\n",
    "def train_envelope(n_parts):\n",
    "    # check how many files in directory\n",
    "    n_files = len(train_data)\n",
    "    if n_parts > n_files:\n",
    "        n_parts = n_files\n",
    "        print(f'Only {n_files} files in directory!')\n",
    "    # create single envelopes for every signal\n",
    "    i = 0\n",
    "    single_envelopes = []\n",
    "    while i<n_parts:\n",
    "        data = train_data[i]\n",
    "        data_signal = preprocessing(data)\n",
    "        env = create_single_envelope(data_signal)\n",
    "        single_envelopes.append(env)\n",
    "        i += 1\n",
    "    # combine envelopes\n",
    "    combined_envelope = np.mean(single_envelopes, axis=0)   # combine by mean value\n",
    "    return combined_envelope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need a method to check, if a bearing is good or not using the envelope we just created. We are calculate mean and standard deviation of the envelope and create a threshold value for good bearings. We can now compare the mean of a test bearing against the threshold value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check, if parts are good\n",
    "def check_parts(data_test_parts, envelope):\n",
    "    # calculate mean and standard deviation of envelope as reference\n",
    "    mean_envelope = np.mean(envelope)\n",
    "    std_envelope = np.std(envelope)\n",
    "    # threshold value for good parts\n",
    "    z = st.norm.ppf((1+CONFIDENCE_LVL)/2)\n",
    "    threshold = mean_envelope + z*std_envelope\n",
    "    \n",
    "    data_bad_parts = []\n",
    "    indexes_bad_parts = []\n",
    "    # check test parts\n",
    "    for idx, test_part in enumerate(data_test_parts):\n",
    "        mean_test_part = np.mean(test_part)\n",
    "        if mean_test_part > threshold:\n",
    "            data_bad_parts.append(test_part)\n",
    "            indexes_bad_parts.append(idx)\n",
    "    return data_bad_parts, indexes_bad_parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test our system using the training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create envelope\n",
    "envelope = train_envelope(400)\n",
    "# read test data\n",
    "data_test_parts = []\n",
    "for data in test_data:\n",
    "    data = preprocessing(data)\n",
    "    data_test_parts.append(data)\n",
    "\n",
    "# check test parts\n",
    "data_bad_parts, indexes_bad_parts = check_parts(data_test_parts, envelope)\n",
    "print(f'{len(data_bad_parts)} bad parts were identified.')\n",
    "print(f'Indexes of bad parts: {indexes_bad_parts}')\n",
    "\n",
    "# visualization\n",
    "for data_bad_part in data_bad_parts:\n",
    "    plt.plot(data_bad_part, c='grey', alpha=0.5)\n",
    "plt.plot(envelope, label='envelope', linestyle='--', c='red')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
